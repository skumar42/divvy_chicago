{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Divvy Bikes Trips Forecast for Given Weather Conditions\n",
    "\n",
    "\n",
    "### Project Description :\n",
    "Divvy is Chicago’s bike share system with thousands of bikes available at more than 580+ stations and 5,800+ bikes across the region. We want to suggest Chicago’s department of transportation about the better management of city transportation system by suggesting the patterns and frequencies about the usage of the Divvy bikes based on city weather. Divvy bike are provided under transportation department of Chicago. Our analysis of bike movements and frequencies will help in suggesting better Divvy bikes kiosk placement in future, considering city dynamics and mobility with respect to current time and weather conditions. When we combine Divvy bike’s daily trip data with Chicago’s weather data, it can be an indicator of commuting habit and frequency of people as well as usage of bikes over any given time with respect to changes in weather. Their usage preferences and frequency might give a sneak peek into indicators for city transportation and mobility.\n",
    "\n",
    "\n",
    "More details about Divvy can be found @ https://www.cityofchicago.org/city/en/depts/cdot.html\n",
    "\n",
    "Divvy bike trips data @ http://www.divvybikes.com/data\n",
    "\n",
    "Chicago's weather History @ https://www.wunderground.com/history/airport/KORD/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Approach and Analysis\n",
    "\n",
    "The data has several continuous and categorical features, so we have dealt with it in two different notebooks.\n",
    "Kindly launch and execute each to see the output.\n",
    "\n",
    "Preprocessing of data includes, dropping features based on missing values (where missing data is more than 50%), scaling of continuous features, binarization of categorical features and removal of outliers from target data.\n",
    "\n",
    "We have done many experiments and our findings indicate that our continuous features alone gives better result as compared to continuous and categorical features both taken together in data set.\n",
    "\n",
    "#### There are three main reasons for above observation:\n",
    "\n",
    "1. Many of the values in categorical features are missing in original data set, this compelled us to try replicating features based on most probabale values, just to study and analyse their effects.\n",
    "\n",
    "2. Limitation on sampled data is another factor, we have analysed data available only for one quarter that does not capture every possible information required to predict the rides correctly.\n",
    "\n",
    "3. Target variable (divvy bike trip counts) may or may not be effected by special events (like ball game, special events in vicinity etc.), which can give rise to incorrect correlation between the predictiion and real data.\n",
    "\n",
    "We have tried two baseline models for regression as our basic model(Mean/Median).\n",
    "\n",
    "#### Other models that we have tried are:\n",
    "\n",
    "1) Linear regression\n",
    "\n",
    "2) Ridge regression\n",
    "\n",
    "3) Lasso regression\n",
    "\n",
    "\n",
    "4) Linear regression(polynomial feature of degree 2)\n",
    "\n",
    "5) Ridge regression(ploynomial feature of degree 2)\n",
    "\n",
    "6) Lasso regression(ploynomial feature of degree 2)\n",
    "\n",
    "\n",
    "7) Linear regression(polynomial feature of degree 3)\n",
    "\n",
    "8) Ridge regression(ploynomial feature of degree 3)\n",
    "\n",
    "9) Lasso regression(ploynomial feature of degree 3)\n",
    "\n",
    "\n",
    "10) Ridge and Lasso with different values of alpha for each model.\n",
    "\n",
    "#### Experimenting with continuous and categorical features together predicts target value as negative sometimes, to avoid this we have tried two possible approaches:\n",
    "\n",
    "1) Every negative predicted value will be set as zero.\n",
    "\n",
    "2) Change the scale of target variable to logarithm and scale back after prediction.\n",
    "\n",
    "Both approcahes mentioned above gave only slight improvement on performace of model, so we dropped these two approaches.\n",
    "\n",
    "We are able to beat the baseline performance using only continuous features (It can be verified with Notebook1).\n",
    "\n",
    "#### Our experiments were to find the correlation between weather and divvy bike trip counts, as have taken data from different sources, which may not be completely correlated so, we have observed that the X is not able to predict Y completely all the time. But, still we were able to beat the baseline performance.\n",
    "\n",
    "\n",
    "#### Description of the Notebooks:\n",
    "\n",
    "Notebook 1 : It deals only with continous features in data set. [Continuous Features](approach/evaluation_continuous.ipynb)\n",
    "\n",
    "Notebook 2 : It deals with continuous as well as categorical features. [Continuous and Categorical Features](approach/evaluation_continuous_categorical.ipynb)\n",
    "\n",
    "\n",
    "Both notebooks cover all the points mentioned on blackboard, the only difference is in input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Major Tasks:\n",
    "\n",
    "\n",
    "1. Baseline Regression\n",
    "\n",
    "2. Ridge Regression\n",
    "\n",
    "3. Data Sampling for Divvy Bikes\n",
    "\n",
    "4. Performance indicator\n",
    "\n",
    "5. Polynomial features implementation\n",
    "\n",
    "6. Linear Regression\n",
    "\n",
    "7. Data colelction for weather\n",
    "\n",
    "8. Features scaling\n",
    "\n",
    "9. Outliers removal\n",
    "\n",
    "10. Feature experiments with continuous data\n",
    "\n",
    "11. Lasso Regression\n",
    "\n",
    "12. Data consolidation and preparation for divvy bikes and weather\n",
    "\n",
    "13. Binarizer for catagorical values\n",
    "\n",
    "14. Feature experimentation for categorical features\n",
    "\n",
    "15. Target variable scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Evaluation : \n",
    "\n",
    "#### Legends: Meaning of \"Train\" in documents is \"whole data\" and CV10 means 10 Fold Cross Validation.\n",
    "\n",
    "#### Mean of target is : [9.3422459893]\n",
    "\n",
    "We have tried different values of alpha, a snapshot of alpha = 0.5 is below.\n",
    "\n",
    "1) BaseLine Mean :\n",
    "\n",
    "[Train] : [RSME : 12.9547857905], [R^2 : 0.000]\n",
    "\n",
    "[Cv10]  : [RSME : 12.9573388654], [R^2 : -0.292]\n",
    "\n",
    "\n",
    "2) BaseLine Median :\n",
    "\n",
    "[Train] : [RSME : 14.4239578156], [R^2 : -0.240]\n",
    "\n",
    "[Cv10]  : [RSME : 14.4239578156], [R^2 : -0.302]  \n",
    "\n",
    "\n",
    "3) Linear regression :\n",
    "\n",
    "[Train] : [RSME : 9.39246473384], [R^2 : 0.474]\n",
    "\n",
    "[Cv10]  : [RSME : 9.42400000000], [R^2 : 0.404]\n",
    "\n",
    "\n",
    "4) Ridge regression : [alpha = 0.5]\n",
    "\n",
    "[Train] : [RSME : 9.45096993372], [R^2 : 0.468]\n",
    "\n",
    "[Cv10]  : [RSME : 9.46150000000], [R^2 : 0.392]\n",
    "\n",
    "\n",
    "5) Lasso regression : [alpha = 0.5]\n",
    "\n",
    "[Train] : [RSME : 10.1684547211], [R^2 : 0.384]\n",
    "\n",
    "[Cv10]  : [RSME : 9.98130000000], [R^2 : 0.355]\n",
    "\n",
    "\n",
    "#### Polynomial Feature of Degree 2\n",
    "\n",
    "6) Linear regression : \n",
    "\n",
    "[Train] : [RSME : 9.18191236453], [R^2 : 0.498]\n",
    "\n",
    "[Cv10]  : [RSME : 9.36410000000], [R^2 : 0.3466]\n",
    "\n",
    "7) Ridge regression : [alpha = 2.712] [Best Model]\n",
    "\n",
    "[Train] : [RSME : 9.313], [R^2 : 0.483]\n",
    "\n",
    "[Cv10]  : [RSME : 9.364], [R^2 : 0.4045]\n",
    "\n",
    "8) Lasso regression : [alpha = 0.5]\n",
    "\n",
    "[Train] : [RSME : 10.1130], [R^2 : 0.3905]\n",
    "\n",
    "[Cv10]  : [RSME : 10.1211], [R^2 : 0.3126]\n",
    "\n",
    "\n",
    "\n",
    "#### Polynomial Feature of Degree 3 [alpha = 0.5]\n",
    "\n",
    "9) Linear regression : \n",
    "\n",
    "[Train] : [RSME : 8.78000504516], [R^2 : 0.541]\n",
    "\n",
    "[Cv10]  : [RSME : 11.2402], [R^2 : -0.524]\n",
    "\n",
    "10) Ridge regression : \n",
    "\n",
    "[Train] : [RSME : 9.1249131893], [R^2 : 0.504]\n",
    "\n",
    "[Cv10]  : [RSME : 11.240200000], [R^2 : 0.395]\n",
    "\n",
    "11) Lasso regression : \n",
    "\n",
    "[Train] : [RSME : 10.113000000], [R^2 : 0.390]\n",
    "\n",
    "[Cv10]  : [RSME : 10.136900000], [R^2 : 0.3126]\n",
    "\n",
    "12) Ridge and Lasso with different values of alpha for each model : \n",
    "\n",
    "Please refer notebook for various values of alpha."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion And Learning : \n",
    "Using linear features are simple and less tend to over fit the data, going for higehr polynomial degree results in better performnace over train data but over cross validation it results in more and more error because of overfitting over the train data.\n",
    "Using graphs and performance indicator helped us to figure out overfitting and find out best model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Best Model Performance Summary : (Continuous Features)\n",
    "\n",
    "#### Best Model : Ridge Regression polynomial degree 2, for alpha = 2.712\n",
    "\n",
    "#### Table for RMSE:\n",
    "\n",
    "|Model|     Baseline(Mean)    |     Baseline(Median)    |   Best model   |\n",
    "|-----|-----------------------|-------------------------|----------------|\n",
    "|Train|     12.954            |       14.423            |    9.3133      |\n",
    "|CV10 |     12.957            |       14.424            |    9.3802      |\n",
    "\n",
    "\n",
    "#### Table for R2 Score\n",
    "\n",
    "|Model|     Baseline(Mean)    |     Baseline(Median)    |   Best model   |\n",
    "|-----|-----------------------|-------------------------|----------------|\n",
    "|Train|        0              |       -.240             |     0.400      |\n",
    "|CV10 |       -.292           |       -.302             |     0.404      |\n",
    "\n",
    "\n",
    "\n",
    "### Important Features  (In descending order of coefficients) :\n",
    "|Coefficient|     Feature Name    |\n",
    "|-----------|---------------------|\n",
    "|13.277     |Dew_PointF           |\n",
    "|3.319      |WindDirDegrees       |\n",
    "|1.266      |Wind_SpeedMPH        |\n",
    "|0.000      |TemperatureF         |\n",
    "|-0.252     |VisibilityMPH        |\n",
    "|-1.265     |Humidity             |\n",
    "|-7.558     |Sea_Level_PressureIn |\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
